{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "taxi_trips = pd.read_parquet('random_taxi_trips_sample.parquet')\n",
    "taxi_trips['start_datetime'] = pd.to_datetime(taxi_trips['start_datetime'])\n",
    "taxi_trips['end_datetime'] = pd.to_datetime(taxi_trips['end_datetime'])\n",
    "\n",
    "spec_events = pd.read_csv('events_december_2012.csv')\n",
    "spec_events['start_datetime'] = pd.to_datetime(spec_events['start_datetime'])\n",
    "spec_events['end_datetime'] = pd.to_datetime(spec_events['end_datetime'])\n",
    "spec_event_types = [None] * len(taxi_trips) \n",
    "taxi_trips['Event Type'] = spec_event_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merges special events with taxi trips dataset for datetime window overlaps/intersections and coordinates within +/- a 500m threshold distance/proximity to taxi trip coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_in_batches(spec_events, taxi_trips, batch_size=1000):\n",
    "    results = [] \n",
    "    num_batches = int(np.ceil(len(taxi_trips) / batch_size))\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        # Process a managable number of rows to prevent RAM error from Cartesian product \n",
    "        batch = taxi_trips.iloc[i * batch_size:(i + 1) * batch_size]\n",
    "        batch['key'] = 0\n",
    "        temp_spec_events = spec_events.copy()\n",
    "        temp_spec_events['key'] = 0\n",
    "        combined = pd.merge(temp_spec_events, batch, on='key').drop('key', axis=1)\n",
    "        \n",
    "        # Keep even data instance if time intervals of trip and event overlap or intersect\n",
    "        combined = combined[(np.maximum(combined['start_datetime_x'], combined['start_datetime_y']) <= \n",
    "                             np.minimum(combined['end_datetime_x'], combined['end_datetime_y']))]\n",
    "\n",
    "        # Record distance between taxi coordinates and event coordinates\n",
    "        combined['lat_diff'] = np.abs(combined['Latitude'] - combined['PU_lat'])\n",
    "        combined['long_diff'] = np.abs(combined['Longitude'] - combined['PU_long'])\n",
    "        \n",
    "        # Keep event data instance if coordinates fall within threshold of trip coordinates (hardcoded, roughly 500m area)\n",
    "        combined = combined[(combined['lat_diff'] <= 0.0045) & (combined['long_diff'] <= 0.0059)]\n",
    "        \n",
    "        # Append only rows meeting both conditions \n",
    "        results.append(combined)\n",
    "        file_name = f\"special_merged_{i}.parquet\"\n",
    "        combined.to_parquet(file_name, index=False)\n",
    "\n",
    "        # Call garbage collection to free up memory for next batch to be processed\n",
    "        del combined, batch, temp_spec_events\n",
    "        gc.collect()\n",
    "\n",
    "    return pd.concat(results) \n",
    "\n",
    "processed_data = process_in_batches(spec_events, taxi_trips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all batch-processed Parquet files into a single DataFrame \n",
    "parquet_files = glob.glob('special_merged/*.parquet')\n",
    "all_events = pd.concat([pd.read_parquet(file) for file in parquet_files], ignore_index=True)\n",
    "\n",
    "# Merge the all filtered events with merged 'weather_taxi_pop' dataset on shared\n",
    "# taxi_trips dataset feature names/values\n",
    "all_events.rename(columns={'start_datetime_y': 'start_datetime',\n",
    "                           'end_datetime_y': 'end_datetime',\n",
    "                           'Event Type_x': 'Event Type'}, inplace=True)\n",
    "\n",
    "# Merge the all filtered events with merged 'weather_taxi_pop' dataset on shared\n",
    "# taxi_trips dataset feature names/values\n",
    "updated_taxi_trips = pd.merge(taxi_trips,\n",
    "                              all_events[['start_datetime', 'end_datetime', 'trip_distance', 'PU_lat', 'PU_long', 'Event Type']],\n",
    "                              on=['start_datetime', 'end_datetime', 'trip_distance', 'PU_lat', 'PU_long'],\n",
    "                              how='left')\n",
    "\n",
    "# Copy event type data from merged dataset into final dataset column\n",
    "updated_taxi_trips['Event Type'] = updated_taxi_trips.apply(\n",
    "    lambda row: row['Event Type_y'] if pd.notna(row['Event Type_y']) else row['Event Type_x'], axis=1)\n",
    "\n",
    "# Delete extraneous columns, rename columns where needed\n",
    "updated_taxi_trips = updated_taxi_trips.drop(columns=['Event Type_x', 'Event Type_y'])\n",
    "updated_taxi_trips = updated_taxi_trips.rename(columns={'Event Type': 'special_events'})\n",
    "special_events = updated_taxi_trips['special_events']\n",
    "updated_taxi_trips = updated_taxi_trips.drop(columns='special_events')\n",
    "updated_taxi_trips = updated_taxi_trips.drop_duplicates()\n",
    "updated_taxi_trips['special_events'] = special_events\n",
    "\n",
    "updated_taxi_trips.to_parquet('../partial_merge/special_events_merge.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
